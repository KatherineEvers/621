---
title: "621 Homework #3"
author: "Katherine Evers"
date: "3/25/2020"
output: html_document
---

Overview

In this homework assignment, you will explore, analyze and model a data set containing information on crime for various neighborhoods of a major city. Each record has a response variable indicating whether or not the crime rate is above the median crime rate (1) or not (0).

Your objective is to build a binary logistic regression model on the training data set to predict whether the neighborhood will be at risk for high crime levels. You will provide classifications and probabilities for the evaluation data set using your binary logistic regression model. You can only use the variables given to you (or variables that you derive from the variables provided). Below is a short description of the variables of interest in the data set:

 zn: proportion of residential land zoned for large lots (over 25000 square feet) (predictor variable)
 indus: proportion of non-retail business acres per suburb (predictor variable)
 chas: a dummy var. for whether the suburb borders the Charles River (1) or not (0) (predictor variable)
 nox: nitrogen oxides concentration (parts per 10 million) (predictor variable)
 rm: average number of rooms per dwelling (predictor variable)
 age: proportion of owner-occupied units built prior to 1940 (predictor variable)
 dis: weighted mean of distances to five Boston employment centers (predictor variable)
 rad: index of accessibility to radial highways (predictor variable)
 tax: full-value property-tax rate per $10,000 (predictor variable)
 ptratio: pupil-teacher ratio by town (predictor variable)
 black: 1000(Bk - 0.63)2 where Bk is the proportion of blacks by town (predictor variable)
 lstat: lower status of the population (percent) (predictor variable)
 medv: median value of owner-occupied homes in $1000s (predictor variable)
 target: whether the crime rate is above the median crime rate (1) or not (0) (response variable)

Deliverables:
 A write-up submitted in PDF format. Your write-up should have four sections. Each one is described
below. You may assume you are addressing me as a fellow data scientist, so do not need to shy away
from technical details.
 Assigned prediction (probabilities, classifications) for the evaluation data set. Use 0.5 threshold.
 Include your R statistical programming code in an Appendix.
Write Up:

```{r packages, echo = FALSE, warning = FALSE, message = FALSE}
library(ggplot2)
library(dplyr)
library(corrplot)
library(reshape2)
library(tidyr)
library(caret)
library(pROC)
library(gridExtra)
library(pander)
```

## 1. DATA EXPLORATION (25 Points)
Describe the size and the variables in the crime training data set. Consider that too much detail will cause a manager to lose interest while too little detail will make the manager consider that you aren’t doing your job. Some suggestions are given below. Please do NOT treat this as a check list of things to do to complete the assignment. You should have your own thoughts on what to tell the boss. These are just ideas.

a. Mean / Standard Deviation / Median
b. Bar Chart or Box Plot of the data
c. Is the data correlated to the target variable (or to other variables?)
d. Are any of the variables missing and need to be imputed “fixed”?

#### Summary Statistics
```{r}
training_data <- read.csv("crime-training-data_modified.csv", header = TRUE)
```

```{r}
raw = read.csv("crime-training-data_modified.csv",  stringsAsFactors = F)

scaled_data = data.frame(scale(raw[,!names(raw) %in% c("target","chas")]))
scaled_data$chas = raw$chas
scaled_data$target = as.factor(raw$target)
```

```{r}
dim(scaled_data)

sum_data <- summary(scaled_data)
pander(sum_data, split.table = 100, style = 'rmarkdown')

scaled_data %>%
  summarise_all(list(~sum(is.na(.))))

table(training_data$target)

above <- training_data[training_data$target == 1,]
below <- training_data[training_data$target == 0,]

summary(above)
summary(below)
```
```{r}
hist(scaled_data$zn)
hist(scaled_data$indus)
hist(scaled_data$chas)
hist(scaled_data$nox)
hist(scaled_data$rm)
hist(scaled_data$age)
hist(scaled_data$dis)
hist(scaled_data$rad)
hist(scaled_data$tax)
hist(scaled_data$ptratio)
hist(scaled_data$lstat)
hist(scaled_data$medv)
```

#### Correlation Matrix
```{r}
data_cor = cor(raw)
corrplot(data_cor, type = "lower")
```
From the correlation matrix and visualization, we see that `indus`, `nox`, `age`, `dis`, `rad`, and `tax` are highly correlated to `target`, with correlation greater than 0.5. The variables `indus`, `nox`, `age`, `rad`, and `tax` are positively correlated while 'dis' is negatively correlated. We will consider these 6 variables of interest and focus our anlysis on them.

```{r}
density_data <- melt(scaled_data[c(1:13)])

ggplot(density_data, aes(value)) + geom_density(fill = "blue") + facet_wrap(~variable, scales = "free")
```
Based on the density plots, we see that `indus`, `rad`, and `tax` are bimodal; `nox` and `dis` are right skewed; and `age` is left skewed.

```{r}
plot1 <- ggplot(scaled_data, aes(x = target, y = indus, group=target)) + geom_boxplot(aes(fill = target)) + coord_flip()
plot2 <- ggplot(scaled_data, aes(x = target, y = nox, group=target)) + geom_boxplot(aes(fill = target)) + coord_flip()
plot3 <- ggplot(scaled_data, aes(x = target, y = age, group=target)) + geom_boxplot(aes(fill = target)) + coord_flip()
plot4 <- ggplot(scaled_data, aes(x = target, y = dis, group=target)) + geom_boxplot(aes(fill = target)) + coord_flip()
plot5 <- ggplot(scaled_data, aes(x = target, y = rad, group=target)) + geom_boxplot(aes(fill = target)) + coord_flip()
plot6 <- ggplot(scaled_data, aes(x = target, y = tax, group=target)) + geom_boxplot(aes(fill = target)) + coord_flip()
plot7 <- ggplot(scaled_data, aes(x = target, y = zn, group=target)) + geom_boxplot(aes(fill = target)) + coord_flip()
plot8 <- ggplot(scaled_data, aes(x = target, y = rm, group=target)) + geom_boxplot(aes(fill = target)) + coord_flip()
plot9 <- ggplot(scaled_data, aes(x = target, y = ptratio, group=target)) + geom_boxplot(aes(fill = target)) + coord_flip()
plot10 <- ggplot(scaled_data, aes(x = target, y = lstat, group=target)) + geom_boxplot(aes(fill = target)) + coord_flip()
plot11 <- ggplot(scaled_data, aes(x = target, y = medv, group=target)) + geom_boxplot(aes(fill = target)) + coord_flip()
plot12 <- ggplot(scaled_data, aes(x = target, y = chas, group=target)) + geom_boxplot(aes(fill = target)) + coord_flip()

grid.arrange(plot1,plot2,plot3,plot4,plot5,plot6,plot7,plot8,plot9,plot10,plot11,plot12, ncol=3)


plot1 <- ggplot(training_data, aes(x= "", y=indus)) + geom_boxplot()  + stat_summary(fun.y=mean,col = 'red',geom='point') + 
 theme_minimal() + theme(axis.title.x=element_blank(), axis.title.y=element_blank(), panel.border=element_rect(fill=NA)) + ggtitle("Indus")

plot2 <- ggplot(training_data, aes(x= "", y=nox)) + geom_boxplot()  + stat_summary(fun.y=mean,col = 'red',geom='point') + 
  theme_minimal() +  theme(axis.title.x=element_blank(), axis.title.y=element_blank(), panel.border=element_rect(fill=NA)) + ggtitle("nox")

plot3 <- ggplot(training_data, aes(x= "", y=age)) + geom_boxplot()  + stat_summary(fun.y=mean,col = 'red',geom='point') + 
  theme_minimal() +  theme(axis.title.x=element_blank(), axis.title.y=element_blank(), panel.border=element_rect(fill=NA)) + ggtitle("age")

plot4 <- ggplot(training_data, aes(x= "", y=dis)) + geom_boxplot()  + stat_summary(fun.y=mean,col = 'red',geom='point') + 
  theme_minimal() +  theme(axis.title.x=element_blank(), axis.title.y=element_blank(), panel.border=element_rect(fill=NA)) + ggtitle("dis")

plot5 <- ggplot(training_data, aes(x= "", y=rad)) + geom_boxplot()  + stat_summary(fun.y=mean,col = 'red',geom='point') + 
  theme_minimal() +  theme(axis.title.x=element_blank(), axis.title.y=element_blank(), panel.border=element_rect(fill=NA)) + ggtitle("rad")

plot6 <- ggplot(training_data, aes(x= "", y=tax)) + geom_boxplot()  + stat_summary(fun.y=mean,col = 'red',geom='point') + 
  theme_minimal() +  theme(axis.title.x=element_blank(), axis.title.y=element_blank(), panel.border=element_rect(fill=NA)) + ggtitle("tax")

grid.arrange(plot1,
plot2,
plot3,
plot4,
plot5,
plot6, ncol=3)

```

The boxplots show how the data is spread for each of the variables of interest listed in the dataset. We see that the variables have a large amount of variance among each other and for each binary value of `target`.

```{r}
#long <- melt(training_data, id.vars= “target”)%>% dplyr::filter(variable != “chas”) %>% mutate(target = as.factor(target))

#ggplot(data = long, aes(x = variable, y = value)) + geom_boxplot(aes(fill = target)) + facet_wrap( ~ variable, scales = “free”)

training_data[c(2, 4, 6:9)] %>%
  gather() %>%
  ggplot(aes(value)) +
    facet_wrap(~key, scales = "free") +
    geom_histogram() + theme(axis.title.x=element_blank(), axis.title.y=element_blank())
```

```{r}
fit <- lm(formula = training_data$target ~ ., data = training_data[c(2, 4, 6:9)][complete.cases(training_data[c(2, 4, 6:9)]),])
summary(fit) # show result

nrow(training_data[c(2, 4, 6:9)][complete.cases(training_data[c(2, 4, 6:9)]),])
 
```



## 2. DATA PREPARATION (25 Points)
Describe how you have transformed the data by changing the original variables or creating new variables. If you did transform the data or create new variables, discuss why you did this. Here are some possible transformations.

a. Fix missing values (maybe with a Mean or Median value)
b. Create flags to suggest if a variable was missing
c. Transform data by putting it into buckets
d. Mathematical transforms such as log or square root (or use Box-Cox)
e. Combine variables (such as ratios or adding or multiplying) to create new variables

To reduce the effect of skew on the model, logistic transformations will be performed on `nox`,`dis`, and `age`.

```{r}
transformation <- training_data %>% mutate_at(c("nox", "dis", "age", "lstat", "medv", "ptratio"), log)
```


3. BUILD MODELS (25 Points)
Using the training data, build at least three different binary logistic regression models, using different variables (or the same variables with different transformations). You may select the variables manually, use an approach such
as Forward or Stepwise, use a different approach, or use a combination of techniques. Describe the techniques you used. If you manually selected a variable for inclusion into the model or exclusion into the model, indicate why this was done.
Be sure to explain how you can make inferences from the model, as well as discuss other relevant model output.
Discuss the coefficients in the models, do they make sense? Are you keeping the model even though it is counter
intuitive? Why? The boss needs to know.

```{r}
model_1 <- glm(target ~ indus + nox + age + dis + rad + tax, family = binomial(link = "logit"), data = training_data)

summary(model_1) #AIC: 230.88 r2: 0.6642144

model_2 <- glm(target ~ nox + age + rad + tax, family = binomial(link = "logit"), data = training_data)

summary(model_2) #AIC: 230.44 0.6586884

#All variables
model_3 <- glm(target ~ ., family = binomial(link = "logit"), data = training_data)

summary(model_3) #AIC: 218.05

model_4 <- glm(target ~ zn + nox + age + dis + rad + tax + ptratio + medv, family = binomial(link = "logit"), data = training_data)

summary(model_4) # AIC: 215.32 0.6944879

#results <- rbind(results,tibble(model = "density models",
                                #predictors = 9,
                                #F1 = cm$byClass[7],
                                #deviance = density_models$deviance,
                                #r2 = 1 - model_5$deviance / model_5$null.deviance,
                                #aic = density_models$aic))

#transformed data
model_1 <- glm(target ~ indus + nox + age + dis + rad + tax, family = binomial(link = "logit"), data = transformation)

summary(model_1) #AIC: 232.91 0.6610585

model <- glm(target ~ nox + dis + rad + tax, family = binomial(link = "logit"), data = transformation)

summary(model) 

#AIC: 230.14 0.6591609

#transformed All variables
#model_7 <- glm(target ~ ., family = binomial(link = "logit"), data = transformation)
#summary(model_7)

#model_8 <- glm(target ~ nox + dis + rad + tax + ptratio+lstat+medv, family = binomial(link = "logit"), data = transformation)
#summary(model_8) #AIC: 219.04
```



4. SELECT MODELS (25 Points)
Decide on the criteria for selecting the best binary logistic regression model. Will you select models with slightly
worse performance if it makes more sense or is more parsimonious? Discuss why you selected your models.

For the binary logistic regression model, will you use a metric such as log likelihood, AIC, ROC curve, etc.? Using
the training data set, evaluate the binary logistic regression model based on (a) accuracy, (b) classification error
rate, (c) precision, (d) sensitivity, (e) specificity, (f) F1 score, (g) AUC, and (h) confusion matrix. Make predictions
using the evaluation data set.

```{r}
par(mfrow = c(2, 2))
plot(model)

hist(model$residuals)
qqnorm(model$residuals)
qqline(model$residuals)
```

#### Test Model

```{r}

target <- transformation %>% select(-target)

test_results <- predict(model, newdata = target, type = "response")

results <- bind_cols(transformation, data.frame(scored_target = test_results)) %>% 
    mutate(scored_target = if_else(scored_target > 0.5, 1, 0)) #%>% print

cm <- confusionMatrix(as.factor(results$scored_target), as.factor(results$target), positive = "1", mode = "everything") %>% print 

#Accuracy : 0.8584 AUC:0.84

curveRoc <- roc(results$target, results$scored_target)
curveRoc$auc



plot(curveRoc[[1]], 
     main = "ROC Curve",
     xlab = "False Positive Rate",
     ylab = "True Positive Rate")


#plot(curveRoc, legacy.axes = T, main = "pROC")

#roc(df$target, df$scored_target, plot = TRUE, legacy.axes = TRUE, print.auc = TRUE)




#predict2 <- predict(model_6, newdata=crime_eval1, type=“response”) summary(predict2) predict11 <- ifelse(predict2 > 0.5, 1, 0) table(predict11)

```

```{r}
transformation$target = as.factor(transformation$target)

set.seed(100)
scaled_index = createDataPartition(transformation$target, p = 0.8, list = F)
scaled_train = transformation[scaled_index,]
scaled_test = transformation[-scaled_index,]


caret_model = train(
  form = target~.,
  data = scaled_train,
  trControl = trainControl(method = "cv", number = 5),
  method = "glm",
  family = "binomial"
)


caret_model

glm_net = train(
  target ~ . ^ 2, data = scaled_train,
  method = "glmnet",
  trControl = trainControl(method = "cv", number = 5),
  tuneLength = 10
)

caret_results = predict(model, newdata = scaled_test, type = "response")

caret_results$predictions = 0
caret_results$predictions[caret_results$`1` >= 0.5] = 1
caret_results$predictions = as.factor(caret_results$predictions)
caret_results

caret::confusionMatrix(caret_results$predictions,scaled_test$target, positive = "1")
```





